{
  "id": "3",
  "title": "Optimizing Database Queries for Performance",
  "slug": "optimizing-database-queries-performance",
  "excerpt": "Practical strategies for improving database query performance in PostgreSQL and MongoDB applications.",
  "coverImage": "https://images.unsplash.com/photo-1544383835-bda2bc66a55d?w=800&h=600&fit=crop",
  "publishedAt": "2024-10-12",
  "readTime": "10 min read",
  "category": "Database",
  "tags": ["PostgreSQL", "MongoDB", "Performance", "Optimization"],
  "author": "Alex Morgan",
  "featured": false,
  "content": "# Optimizing Database Queries for Performance\n\nDatabase performance is crucial for application scalability. In this post, we'll cover indexing strategies, query optimization techniques, and best practices for both PostgreSQL and MongoDB.\n\n## Understanding Query Performance\n\nBefore optimizing, you need to measure. Both PostgreSQL and MongoDB provide tools to analyze query performance.\n\n### PostgreSQL: EXPLAIN ANALYZE\n\n```sql\nEXPLAIN ANALYZE\nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id, u.name\nORDER BY order_count DESC\nLIMIT 10;\n```\n\n### MongoDB: explain()\n\n```javascript\ndb.users.find({ createdAt: { $gt: new Date('2024-01-01') } })\n  .sort({ orderCount: -1 })\n  .limit(10)\n  .explain('executionStats');\n```\n\n## Indexing Strategies\n\n### PostgreSQL Indexes\n\n#### B-tree Index (Default)\n```sql\n-- Single column index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Composite index (order matters!)\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at);\n\n-- Partial index\nCREATE INDEX idx_active_users ON users(email) WHERE active = true;\n```\n\n#### GIN Index (for arrays and JSON)\n```sql\n-- For JSONB columns\nCREATE INDEX idx_users_metadata ON users USING GIN(metadata);\n\n-- For array columns\nCREATE INDEX idx_posts_tags ON posts USING GIN(tags);\n```\n\n### MongoDB Indexes\n\n```javascript\n// Single field index\ndb.users.createIndex({ email: 1 });\n\n// Compound index\ndb.orders.createIndex({ userId: 1, createdAt: -1 });\n\n// Text index for full-text search\ndb.posts.createIndex({ title: 'text', content: 'text' });\n\n// Geospatial index\ndb.locations.createIndex({ coordinates: '2dsphere' });\n```\n\n## Query Optimization Techniques\n\n### 1. Select Only What You Need\n\n```sql\n-- ❌ Bad: Selecting all columns\nSELECT * FROM users WHERE id = 1;\n\n-- ✅ Good: Select specific columns\nSELECT id, name, email FROM users WHERE id = 1;\n```\n\n### 2. Use Proper JOIN Types\n\n```sql\n-- ❌ Bad: Unnecessary LEFT JOIN\nSELECT u.name, o.total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id;\n\n-- ✅ Good: INNER JOIN when you only need matching records\nSELECT u.name, o.total\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id;\n```\n\n### 3. Avoid N+1 Queries\n\n```javascript\n// ❌ Bad: N+1 query problem\nconst users = await User.findAll();\nfor (const user of users) {\n  user.orders = await Order.findAll({ where: { userId: user.id } });\n}\n\n// ✅ Good: Use eager loading\nconst users = await User.findAll({\n  include: [{ model: Order }]\n});\n```\n\n### 4. Use Pagination\n\n```sql\n-- ❌ Bad: Loading all records\nSELECT * FROM posts ORDER BY created_at DESC;\n\n-- ✅ Good: Paginate results\nSELECT * FROM posts \nORDER BY created_at DESC \nLIMIT 20 OFFSET 0;\n\n-- ✅ Better: Cursor-based pagination\nSELECT * FROM posts \nWHERE created_at < '2024-01-01' \nORDER BY created_at DESC \nLIMIT 20;\n```\n\n## Advanced PostgreSQL Optimization\n\n### Materialized Views\n\n```sql\n-- Create materialized view for expensive queries\nCREATE MATERIALIZED VIEW user_stats AS\nSELECT \n  u.id,\n  u.name,\n  COUNT(DISTINCT o.id) as order_count,\n  SUM(o.total) as total_spent,\n  AVG(o.total) as avg_order_value\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Create index on materialized view\nCREATE INDEX idx_user_stats_order_count ON user_stats(order_count);\n\n-- Refresh when needed\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_stats;\n```\n\n### Query Caching with Redis\n\n```javascript\nconst redis = require('redis');\nconst client = redis.createClient();\n\nasync function getUserStats(userId) {\n  const cacheKey = `user:${userId}:stats`;\n  \n  // Try cache first\n  const cached = await client.get(cacheKey);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n  \n  // Query database\n  const stats = await db.query(`\n    SELECT \n      COUNT(DISTINCT o.id) as order_count,\n      SUM(o.total) as total_spent\n    FROM orders o\n    WHERE o.user_id = $1\n  `, [userId]);\n  \n  // Cache for 1 hour\n  await client.setex(cacheKey, 3600, JSON.stringify(stats));\n  \n  return stats;\n}\n```\n\n### Connection Pooling\n\n```javascript\nconst { Pool } = require('pg');\n\nconst pool = new Pool({\n  host: 'localhost',\n  database: 'myapp',\n  max: 20, // Maximum pool size\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 2000,\n});\n\n// Use pool for queries\nconst result = await pool.query('SELECT * FROM users WHERE id = $1', [userId]);\n```\n\n## MongoDB Optimization\n\n### Aggregation Pipeline Optimization\n\n```javascript\n// ❌ Bad: Filtering after grouping\ndb.orders.aggregate([\n  {\n    $group: {\n      _id: '$userId',\n      total: { $sum: '$amount' }\n    }\n  },\n  {\n    $match: { total: { $gt: 1000 } }\n  }\n]);\n\n// ✅ Good: Filter before grouping\ndb.orders.aggregate([\n  {\n    $match: { amount: { $gt: 100 } } // Filter early\n  },\n  {\n    $group: {\n      _id: '$userId',\n      total: { $sum: '$amount' }\n    }\n  },\n  {\n    $match: { total: { $gt: 1000 } }\n  }\n]);\n```\n\n### Projection to Reduce Data Transfer\n\n```javascript\n// ❌ Bad: Fetching entire documents\ndb.users.find({ active: true });\n\n// ✅ Good: Project only needed fields\ndb.users.find(\n  { active: true },\n  { name: 1, email: 1, _id: 0 }\n);\n```\n\n### Covered Queries\n\n```javascript\n// Create compound index\ndb.users.createIndex({ email: 1, name: 1, active: 1 });\n\n// Query is covered by index (no document lookup needed)\ndb.users.find(\n  { email: 'user@example.com' },\n  { name: 1, active: 1, _id: 0 }\n);\n```\n\n## Real-World Example: E-commerce Analytics\n\n```sql\n-- Slow query: Calculate daily sales\nSELECT \n  DATE(created_at) as date,\n  COUNT(*) as order_count,\n  SUM(total) as revenue\nFROM orders\nWHERE created_at >= NOW() - INTERVAL '30 days'\nGROUP BY DATE(created_at)\nORDER BY date DESC;\n\n-- Optimized with index\nCREATE INDEX idx_orders_created_at ON orders(created_at);\n\n-- Further optimized with materialized view\nCREATE MATERIALIZED VIEW daily_sales AS\nSELECT \n  DATE(created_at) as date,\n  COUNT(*) as order_count,\n  SUM(total) as revenue,\n  AVG(total) as avg_order_value\nFROM orders\nGROUP BY DATE(created_at);\n\nCREATE INDEX idx_daily_sales_date ON daily_sales(date);\n\n-- Refresh nightly via cron\nREFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales;\n```\n\n## Monitoring and Maintenance\n\n### PostgreSQL\n\n```sql\n-- Find slow queries\nSELECT \n  query,\n  calls,\n  total_time,\n  mean_time,\n  max_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Find missing indexes\nSELECT \n  schemaname,\n  tablename,\n  attname,\n  n_distinct,\n  correlation\nFROM pg_stats\nWHERE schemaname = 'public'\nORDER BY n_distinct DESC;\n\n-- Analyze table statistics\nANALYZE users;\n\n-- Vacuum to reclaim space\nVACUUM ANALYZE users;\n```\n\n### MongoDB\n\n```javascript\n// Find slow queries\ndb.system.profile.find({\n  millis: { $gt: 100 }\n}).sort({ ts: -1 }).limit(10);\n\n// Get collection stats\ndb.users.stats();\n\n// Analyze index usage\ndb.users.aggregate([\n  { $indexStats: {} }\n]);\n```\n\n## Best Practices Checklist\n\n- ✅ Add indexes on frequently queried columns\n- ✅ Use composite indexes for multi-column queries\n- ✅ Avoid SELECT * in production code\n- ✅ Implement connection pooling\n- ✅ Use prepared statements to prevent SQL injection\n- ✅ Cache frequently accessed data\n- ✅ Monitor slow queries regularly\n- ✅ Use pagination for large result sets\n- ✅ Optimize JOIN operations\n- ✅ Regular VACUUM and ANALYZE (PostgreSQL)\n- ✅ Keep statistics up to date\n- ✅ Use read replicas for read-heavy workloads\n\n## Conclusion\n\nDatabase optimization is an ongoing process. Start by measuring, identify bottlenecks, and apply targeted optimizations. Remember:\n\n- Indexes speed up reads but slow down writes\n- Cache strategically, not everything\n- Monitor query performance in production\n- Test optimizations with realistic data volumes\n\nWith these techniques, you can significantly improve your application's database performance and scalability.\n"
}
